{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.lib.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing\n",
    "import scipy.signal\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio has been recorded with a sampling rate of 44100. Let's load all of the data into a pandas DataFrame so that we don't have to load the audio everytime we want to train on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 604 ms, sys: 792 ms, total: 1.4 s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "anno = pd.read_csv('data/annotations.csv')\n",
    "\n",
    "audio = []\n",
    "\n",
    "for _, row in anno.iterrows():\n",
    "    recording, _ = librosa.load(f'data/audio/{row.fn}', sr=None)\n",
    "    audio.append(recording)\n",
    "    \n",
    "anno['audio'] = audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only annotations we have available for this dataset are whether the recording contains an orca call or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>label</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.wav</td>\n",
       "      <td>call</td>\n",
       "      <td>[0.07357788, 0.09561157, 0.072021484, 0.019134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.wav</td>\n",
       "      <td>call</td>\n",
       "      <td>[-0.051696777, -0.023529053, 0.014511108, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.wav</td>\n",
       "      <td>call</td>\n",
       "      <td>[-0.0035552979, -6.1035156e-05, -0.011566162, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.wav</td>\n",
       "      <td>call</td>\n",
       "      <td>[0.0016479492, 0.024032593, 0.032470703, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.wav</td>\n",
       "      <td>call</td>\n",
       "      <td>[-0.030548096, -0.046203613, -0.029937744, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fn label                                              audio\n",
       "0  0.wav  call  [0.07357788, 0.09561157, 0.072021484, 0.019134...\n",
       "1  1.wav  call  [-0.051696777, -0.023529053, 0.014511108, 0.02...\n",
       "2  2.wav  call  [-0.0035552979, -6.1035156e-05, -0.011566162, ...\n",
       "3  3.wav  call  [0.0016479492, 0.024032593, 0.032470703, -0.00...\n",
       "4  4.wav  call  [-0.030548096, -0.046203613, -0.029937744, 0.0..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All recordings are 4 second long. But the classes are not balanced. We provide an option to balance the classes for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call       398\n",
       "no_call    196\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two versions of the dataset:\n",
    "* **unbalanced** (examples represented in line with counts in the raw dataset)\n",
    "* **balanced** (examples upsampled to count of the most frequently occuring class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping from labels to class idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = anno['label'].unique().tolist()\n",
    "\n",
    "label2idx = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, classes='unbalanced'):\n",
    "        self.examples = df\n",
    "    \n",
    "        assert classes in ['balanced', 'unbalanced']\n",
    "        if classes=='balanced':\n",
    "            max_examples = self.examples['label'].value_counts()[0]\n",
    "            for grp in self.examples.groupby('label'):\n",
    "                example_count = self.examples[self.examples['label'] == grp[0]].shape[0]\n",
    "                while example_count < max_examples:\n",
    "                    self.examples = self.examples.append(self.examples[self.examples['label'] == grp[0]][:max_examples-example_count])\n",
    "                    example_count = self.examples[self.examples['label'] == grp[0]].shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples.iloc[index]\n",
    "        x = example.audio\n",
    "        y = label2idx[example['label']]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.examples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the components in place, let's work on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = anno.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(anno[:int(0.8*anno.shape[0])], classes='balanced')\n",
    "valid_ds = Dataset(anno[int(0.8*anno.shape[0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_call    318\n",
       "call       318\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.examples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now construct the dataloaders to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl: pass\n",
    "for batch in valid_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 176400]), torch.Size([23]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape # we are on the final batch, there were not enough examples to fill it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPP suitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a relatively small number of calls and the callers are not identified. Additionally, there is a high amount of backround noise.\n",
    "\n",
    "This dataset would not lend itself well to CPP analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
